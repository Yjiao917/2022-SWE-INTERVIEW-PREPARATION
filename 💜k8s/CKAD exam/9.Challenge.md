Kubernetes Certification Practice Check 4: Pod Secret
In the sjq namespace, create a secret named xh8jqk7z that stores a generic secret with the key of tkn and the value of hy8szK2iu. Create a pod named server using the httpd:2.4.39-alpine image and give the pod's container access to the tkn key in the xh8jqk7z secret through an environment variable named SECRET_TKN.

```bash
k create secret generic xh8jqk7z --from-literal=tkn=hy8szK2iu

k run server --image=httpd:2.4.39-alpine --namespace=sjq -o yaml > server.yaml

vim server.yaml
```

```yaml
apiVersion: v1
kind: Pod
spec:
  containers:
  - name: envars-test-container
    env:
     - name: SECRET_TKN
       valueFrom:
         secretKeyRef:
           name: xh8jqk7z
           key: tkn
```




Service Account
Create a service account named inspector in the dwx7eq namespace. Then create a deployment named calins in the same namespace. Use the image busybox:1.31.1 for the only pod container and pass the arguments sleep and 24h to the container. Set the number of replicas to 1. Lastly, make sure that the deployments' pod is using the inspector service account.

```bash

ubuntu@ip-10-0-128-5:~$ k create serviceaccount inspector -n dwx7eq 
# serviceaccount/inspector created
ubuntu@ip-10-0-128-5:~$ k config set-context --current --namespace=dwx7eq
# Context "challenge-context" modified.

ubuntu@ip-10-0-128-5:~$ k create deployment calins --namespace=dwx7eq --image=busybox:1.31.1 --replicas=1 -- sleep 24h
# deployment.apps/calins created

k edit deployment calins

```
```yaml
apiVersion: apps/v1
kind: Deployment
spec:
  template:
    spec:
      containers:
      serviceAccountName: inspector

```


Evictions
The mission-critical deployment in the bk0c2d namespace has been getting evicted when the Kubernetes cluster is consuming a lot of memory. Modify the deployment so that it will not be evicted when the cluster is under memory pressure unless there are higher priority pods running in the cluster (Guaranteed Quality of Service). It is known that the container for the deployment's pod requires and will not use more than 200 milliCPU (200m) and 200 mebibytes (200Mi) of memory.



```yaml

apiVersion: apps/v1
kind: Deployment
spec:
  template:
    spec:
      containers:
      - name: log-aggregator
        resources:
          requests:
            memory: "64Mi"
            cpu: "250m"
          limits:
            memory: "128Mi"
            cpu: "500m"
```

Persisting Data
A legacy application runs via a deployment in the zuc0co namespace. The deployment's pod uses a multi-container pod to convert the legacy application's raw metric output into a format that can be consumed by a metric aggregation system. However, the data is currently lost every time the pod is deleted. Modify the deployment to use a persistent volume claim with 2GiB of storage and access mode of ReadWriteOnce so the data is persisted if the pod is deleted.







```bash
cat <<EOF | k create -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv
  namespace: zuc0co

spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
EOF

cat <<EOF | k create -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvclaim
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 2Gi
EOF

```
```yaml
apiVersion: apps/v1
kind: Deployment
spec:
  template:
    spec:
      containers:
      - args:
        - while true; do date > /metrics/raw.txt; top -n 1 -b >> /metrics/raw.txt;
          sleep 5; done
        command:
        - /bin/sh
        - -c
        image: alpine:3.9.2
        imagePullPolicy: IfNotPresent
        name: app
        volumeMounts:
        - mountPath: /metrics
          name: metrics
        - mountPath: /var
          name: pvclaim

     volumes:
      - emptyDir: {}
        name: metrics
      - name: pvclaim
        persistentVolumeClaim:
          claimName: pvclaim
```



















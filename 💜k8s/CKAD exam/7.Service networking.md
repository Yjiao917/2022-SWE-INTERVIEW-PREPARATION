
Check 1: Create and Configure a Basic Pod
Create a Pod in the red Namespace with the following configuration:

The Pod is named basic
The Pod uses the nginx:stable-alpine-perl image for its only container
Restart the Pod only OnFailure
Ensure port 80 is open to TCP traffic



```bash
k run basic --namespace=red --image=nginx:stable-alpine-perl --restart=OnFailure --port=80

```



Check 2: Expose Pod
Expose the Pod named basic in the red Namespace, ensuring it has the following settings:

Service name is cloudacademy-svc
Service port is 8080
Target port is 80
Service type is ClusterIP


```bash
kubectl expose pod basic --namespace=red --name=cloudacademy-svc --port=8080 --target-port=80 --type=ClusterIP

```




Check 3: Expose existing Deployment
A Deployment named cloudforce has been created in the ca1 Namespace. You must now expose this Deployment as a NodePort based Service using the following settings:

Service name is cloudforce-svc
Service type is NodePort
Service port is 80
NodePort is 32080

```bash

k expose deployment cloudforce --namespace=ca1 --name=cloudforce-svc --port=80 --target-port=32080 --type=NodePort

## note; does not exist `--node-port`

## use vim to edit or `k edit svc cloudforce-svc  --namespace=ca1`
```
```yaml
apiVersion: v1
kind: Service
metadata:
  name: cloudforce-svc
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: NodePort
```


Check 4: Fix Networking Issue
A Deployment named t2 has been created in the skynet Namespace, and has been exposed as a ClusterIP based Service named t2-svc. The t2-svc Service when contacted should return a valid HTTP response, but unfortunately, this is currently not yet the case. Please investigate and fix the problem. When you have applied the fix, run the following command to save the HTTP response:

kubectl run client -n skynet --image=appropriate/curl -it --rm --restart=Never -- curl http://t2-svc:8080 > /home/ubuntu/svc-output.txt


```bash

# Examine both the Deployment and Service, confirming that the Service selector uses the correct pod labels as specified in the Deployment
kubectl describe deployments.apps -n skynet t2
kubectl describe svc -n skynet t2-svc
# Check the t2-svc Service Endpoints are correctly registered - note that no endpoints are currently registered
kubectl get ep -n skynet t2-svc
# Update the t2-svc Service selector to use the correct Deployment pod label (app=t2)
kubectl edit svc -n skynet t2-svc

Labels:            app=t2
Annotations:       <none>
Selector:          app=t2
Port:              <unset>  8080/TCP
TargetPort:        80/TCP

# Check the t2-svc Service Endpoints are now correctly registered
kubectl get ep -n skynet t2-svc
# Curl the updated t2-svc Service and save the HTTP response
kubectl run client -n skynet --image=appropriate/curl -it --rm --restart=Never -- curl http://t2-svc:8080 > /home/ubuntu/svc-output.txt

```



Check 5: Secure Pod Networking
The following two Pods have been created in the sec1 Namespace:

pod1 has the label app=test
pod2 has the label app=client
A NetworkPolicy named netpol1 has also been established in the sec1 Namespace but is currently blocking traffic sent from pod2 to pod1. Update the NetworkPolicy to ensure that pod2 can send traffic to pod1. Ensure the NetworkPolicy is still being applied to pod2 in your solution.

```bash
k set-context --current --namespace=sec1

# Confirm that pod2 traffic sent to pod1 is as stated currently blocked
pod1IP=$(kubectl get pods pod1  -o jsonpath='{.status.podIP}')
kubectl  exec -it pod2 -- ping $pod1IP
# Examine the pod labels for pod1 (receiver) and pod2 (sender) 
kubectl get pods  --show-labels
# Examine the existing NetworkPolicy
kubectl -n sec1 describe netpol netpol1


Name:         netpol1
Namespace:    sec1
Created on:   2024-01-16 22:41:43 +0000 UTC
Labels:       <none>
Annotations:  <none>
Spec:
  PodSelector:     app=test
  Allowing ingress traffic:
    To Port: <any> (traffic allowed to all ports)
    From:
      PodSelector: app=test
  Not affecting egress traffic

# Edit and update the NetworkPolicy with correct ingress pod selector label
kubectl edit -n sec1 netpol netpol1



# Confirm that pod2 can now send traffic to pod1
pod1IP=$(kubectl get pods pod1 -n sec1 -o jsonpath='{.status.podIP}')
kubectl -n sec1 exec -it pod2 -- ping $pod1IP

```


```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-client-to-test
  namespace: <your-namespace>  # Replace with your actual namespace
spec:
  podSelector:
    matchLabels:
      app: test
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: client


  policyTypes:
  - Ingress

```
